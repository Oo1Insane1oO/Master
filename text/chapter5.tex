%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Numerical Optimization \label{chapter:6}}
    In the Variational Monte Carlo method in \Arf{sec:QMC} the essential point
    was to vary a set of \txtit{variational parameters} in order to reach an
    eigenbasis which gives the ground-state energy of the Hamiltonian in
    question. There are many was one could approach this. One way could be to
    wildly guess random parameters and hope for the best, obviously this is a
    poor approach. The more sound approach would be to optimize(minimization
    int the VMC case) the wavefunction using methods from a popular field in
    mathematics called \txttit{numerical optimization}. The two methods used in
    this thesis was the \txtit{Conjugate Gradient method} and a version of the
    \txtit{Adaptive Stochastic Gradient Descent}. The explaining of the
    approaches for numerical optimization is only explained briefly. For a
    better mathematical explanation see >> REF THESE <<

\section{The Optimization Problem \label{sec:the_optimization_problem}}
    We will explain the general approach for minimizing a multi-variate
    function and set the terminology in this section. \\

    The problem in question is the following. Given a continuously
    differentiable function $f:\mathbb{R}^n\rarr\mathbb{R}$, for what set of
    parameters $\{\alpha\}^{n}_{k=1}$ is 
        \begin{equation}
            \nabla_{\alpha} f = \blds{0}
            \label{eq:mincondition}
        \end{equation}
    fulfilled\footnote{$\nabla_{\alpha}=\sum\limits_k\blds{e}_k\prd{\alpha_k}$
    with $\blds{e}_k\in\mathbb{R}^n$ a unit vector along direction $k$.}. This
    means we seek a point $\blds{\alpha}_m$ in the real where variation of the
    value of $f$ is zero. In reality the condition in \Arf{eq:mincondition} is
    only approximate, that is we terminate the search for a minimum if we
    reached a point where the absolute value of $f$ is within a threshold
    $\epsilon$
        \begin{equation}
            \abs{\nabla_{\alpha} f} \leq \epsilon
            \label{eq:minconditionapprox}
        \end{equation}
    One might at this point have the question, wouldn't the condition presented
    in \Arf{eq:minconditionapprox}(and \Arf{eq:mincondition} be valid for a
    maximum as well? The answer is yes, it would. The simple fix to this is to
    define the \txtit{search direction}, more precisely the sign of the search
    direction. The next section explains this in better detail.

\section{Gradient Descent\label{sec:gradient_descent}}
    We defined the optimization problem and defined a simple condition for the
    extremal and mentioned a search direction in the previous section. A search
    direction in our context is a direction $\blds{p}\in\mathbb{R}^n$ which
    points towards $\blds{\alpha}_m$. To find $\blds{p}$ we use the well known
    \txtit{second derivative test} to determine the curvature of $f$. This,
    mentioned qualitatively, means that the gradient of $f$ any point
    $\blds{\alpha}_i$ points towards an extremal and that the negative
    gradient(negative sign) points towards the minimum and the positive
    gradient points towards the maximum. This observation gives a simple rule
    for finding $\blds{\alpha}_m$. Start out with blindly guessing a point
    $\blds{\alpha}_0$ and keep updating the parameters according to the
    following recursive rule
        \begin{equation}
            \blds{\alpha}_k = \blds{\alpha_{k-1}} - \gamma\nabla_{\alpha} f
        \end{equation}
    and terminate the search when \Arf{eq:minconditionapprox} is fulfilled. >>
    REF AND MAKE ALGORITHM <<\footnote{Change the negative sign in front of the
    gradient if a maximum is desired.} \\

    This method of finding the minimum is known as the method of
    \txtit{Gradient Descent} and is the simplest method for finding a minimum.
    The problem however is stability, the termination condition is firstly not
    optimal >> REF THIS << and the step-size $\gamma$ is a constant which can
    give allot of oscillations around minimum as the algorithm might get close
    to the minimum and then \txtit{over-shoot} and go past the minimum point,
    turn around(because the sign changes) and over-shoot again and then keep
    going. Many(seriously many) methods have been devised to account for these
    problems and other. We will contain ourselves with the methods mentioned in
    the introduction of this chapter.

\section{Adaptive Stochastic Gradient
Descent\label{sec:adaptive_stochastic_gradient_descent}}
    Along with the limitations of the method of gradient descent, the
    \txtit{Adaptive Stochastic Gradient Descent} tries to account for those,
    but also takes into account the variance introduced by the stochastic
    nature of the probability distribution. As such many variations of the
    method have been proven to be popular among problems in which the function
    to be minimized is an expectation value.

    
\section{Newtons-Method and Quasi-Newton
Methods\label{sec:newtons-method_and_quasi-newton_methods}}
    Write something...

\section{BFGS\label{sec:BFGS}}
    In the two previous sections we mentioned methods that used the gradient of
    $f$ as the desired search direction. The method known as
    BFGS\footnote{Stands for Broyden-Fletcher-Goldfarb-Shanno algorithm after
    the creators.} we use, in addition to the gradient, the \txtit{Hessian
    matrix} as well (an approximation to it anyways). This method also belongs
    to the class of quasi-Newton methods as it approximates the Hessian with
    the gradient at each step.
