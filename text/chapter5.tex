%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Numerical Optimization \label{chapter:5}}
    In the Variational Monte Carlo method in \Arf{sec:QMC} the essential point
    was to vary a set of \txtit{variational parameters} in order to reach an
    eigenbasis which gives the ground-state energy of the Hamiltonian in
    question. There are many was one could approach this. One way could be to
    wildly guess random parameters and hope for the best, obviously this is a
    poor approach. The more sound approach would be to optimize(minimization
    int the VMC case) the wavefunction using methods from a popular field in
    mathematics called \txtit{numerical optimization}. The two methods used in
    this thesis was the \txtit{Conjugate Gradient method} and a version of the
    \txtit{Adaptive Stochastic Gradient Descent}. The explaining of the
    approaches for numerical optimization is only explained briefly. For a
    better mathematical explanation see >> REF THESE <<

\section{The Optimization Problem \label{sec:the_optimization_problem}}
    We will explain the general approach for minimizing a multi-variate
    function and set the terminology in this section. \\

    The problem in question is the following. Given a continuously
    differentiable function $f:\mathbb{R}^n\rarr\mathbb{R}$, for what set of
    parameters $\{\alpha\}^{n}_{k=1}$ is 
        \begin{equation}
            \nabla_{\alpha} f = \blds{0}
            \label{eq:mincondition}
        \end{equation}
    fulfilled\footnote{$\nabla_{\alpha}=\sum\limits_k\blds{e}_k\prd{\alpha_k}$
    with $\blds{e}_k\in\mathbb{R}^n$ a unit vector along direction $k$.}. This
    means we seek a point $\blds{\alpha}_m$ in the real where variation of the
    value of $f$ is zero. In reality the condition in \Arf{eq:mincondition} is
    only approximate, that is we terminate the search for a minimum if we
    reached a point where the absolute value of $f$ is within a threshold
    $\epsilon$
        \begin{equation}
            \abs{\nabla_{\alpha} f} \leq \epsilon
            \label{eq:minconditionapprox}
        \end{equation}
    One might at this point have the question, wouldn't the condition presented
    in \Arf{eq:minconditionapprox}(and \Arf{eq:mincondition} be valid for a
    maximum as well? The answer is yes, it would. The simple fix to this is to
    define the \txtit{search direction}, more precisely the sign of the search
    direction. The next section explains this in better detail.

\section{Gradient Descent\label{sec:gradient_descent}}
    We defined the optimization problem and defined a simple condition for the
    extremal and mentioned a search direction in the previous section. A search
    direction in our context is a direction $\blds{p}\in\mathbb{R}^n$ which
    points towards $\blds{\alpha}_m$. To find $\blds{p}$ we use the well known
    \txtit{second derivative test} to determine the curvature of $f$. This,
    mentioned qualitatively, means that the gradient of $f$ any point
    $\blds{\alpha}_i$ points towards an extremal and that the negative
    gradient(negative sign) points towards the minimum and the positive
    gradient points towards the maximum. This observation gives a simple rule
    for finding $\blds{\alpha}_m$. Start out with blindly guessing a point
    $\blds{\alpha}_0$ and keep updating the parameters according to the
    following recursive rule
        \begin{equation}
            \blds{\alpha}_n = \blds{\alpha_{n-1}} - \gamma\nabla_{\alpha} f
            \label{eq:gdupdate}
        \end{equation}
    and terminate the search when \Arf{eq:minconditionapprox} is fulfilled. >>
    REF AND MAKE ALGORITHM <<\footnote{Change the negative sign in front of the
    gradient if a maximum is desired.} \\

    This method of finding the minimum is known as the method of
    \txtit{Gradient Descent} and is the simplest method for finding a minimum.
    The problem however is stability, the termination condition is firstly not
    optimal >> REF THIS << and the step-size $\gamma$ is a constant which can
    give allot of oscillations around minimum as the algorithm might get close
    to the minimum and then \txtit{over-shoot} and go past the minimum point,
    turn around(because the sign changes) and over-shoot again and then keep
    going. Many(seriously many) methods have been devised to account for these
    problems and other. We will contain ourselves with the methods mentioned in
    the introduction of this chapter.

\section{Adaptive Stochastic Gradient
Descent\label{sec:adaptive_stochastic_gradient_descent}}
    Along with the limitations of the method of gradient descent, the
    \txtit{Adaptive Stochastic Gradient Descent} tries to account for those,
    but also takes into account the variance introduced by the stochastic
    nature of the probability distribution. As such many variations of the
    method have been proven to be popular among problems in which the function
    to be minimized is an expectation value. The method used in this thesis is
    the one described in \cite{ASGD}. We will give a summary of the method
    here, for a more detailed outline and description see \cite{ASGD}. \\

    Like the gradient descent method the adaptive stochastic gradient descent
    method updates the parameters in the same manner as in \Arf{eq:gdupdate},
    the difference however is that the step $\gamma$ is changed for each
    iteration in accordance to the following
        \begin{equation}
            \begin{aligned}
                \gamma_{n+1} &= \frac{a}{t_{n+1} + A} \\
                t_{n+1} &= \text{max}(t_n + g(X_n), 0) \\
                X_n &= - \nabla f_n \cdot \nabla f_{n+1} \\
                g(x) &= g_{\text{min}} + \frac{g_{\text{max}} -
                g_{\text{min}}}{1 - \frac{g_{\text{max}}}{g_{\text{min}}}
                \me^{-\frac{x}{\omega}}}
            \end{aligned}
        \end{equation}
    The whole idea of the method is that the form of $g$ and the accumulative
    combination of gradient estimations for each step the total error would
    tend quickly to zero, meaning the central element(namely the gradient) in
    the minimization is well behaving. \\

    The main concern with the method is the convergence, although the error in
    the gradient estimations tend towards zero, the step-sizes themselves will
    also be quite small after some iterations. For this reason we use the
    adaptive method with a quasi-Newton method. This means that we start with a
    random guess at the parameters and keep iterating with the quasi-Newton
    method until the norm of the gradient is below some threshold, at which the
    adaptive method is applied from that point and onward till convergence is
    reached.

\section{Newtons-Method and Quasi-Newton
Methods\label{sec:newtons-method_and_quasi-newton_methods}}
    We will here explain briefly \txtit{Newton's method} and
    \txtit{Quasi-Newton} methods as the ideas presented will be used in the
    next section. \\

    Newton's method\cite{linalgDavid}(or Newton-Raphson method) is originally a
    method for finding the zeros of a function. The rule states that given a
    real-valued function $f:\mathbb{R}\rarr\mathbb{R}$ and an initial guess
    $x\in\mathbb{R}$ for the zero-point, recursively find better
    approximations for the zero by
        \begin{equation}
            x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
        \end{equation}
    This method would then within a number iterations find the zero that is
    closest to $x_0$.

    For the optimization problem the condition for a point to be an extremal is
    \Arf{eq:mincondition} meaning, again, that one needs to find the zero of
    the derivative. Newton's method in this case would be
        \begin{equation}
            x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)},\indent n\geq 0
        \end{equation}
    Of course in the real world one might work with multi-variate function, not
    to worry as Newton's method for optimization problems in the multi-variate
    case with $f:\mathbb{R}^n\rarr\mathbb{R}$ (still real-valued) is
        \begin{equation}
            \blds{x}_{n+1} = \blds{x}_n -
            \abs{\blds{H}f(\blds{x}_n)}^{-1}\dot\nabla f(\blds{x}_n),\indent
            n\geq 0
        \end{equation}
    where $\blds{H}$ is the Hessian matrix. One might also introduce a
    step-length multiplied to the Hessian part in order to induce
    conditions\cite{numOptNocWrig} which ensure some stability of the method.
    \\

    Newton's method, in most cases, converges faster(less iterations) towards
    the minimum than gradient descent making it favorable, however the full
    Hessian has to be known. This matrix(or its inverse) is in many cases too
    expensive to compute or difficult to express in closed-form. In these cases
    the class of methods knows as Quasi-Newton methods can be utilized. \\

    Quasi-Newton methods give an estimate of the inverse Hessian by using the
    first derivatives. Introduce the Taylor approximation of $f$ around an
    iteration point $\blds{x}_n$
        \begin{equation}
            f(\blds{x}_k + \blds{s}) \approx f(\blds{x}_k) + \left(\nabla
            f(\blds{x}_k)\right)^T\blds{s} + \frac{1}{2}\blds{s}^T \blds{H}
            \blds{s}
        \end{equation}
    differentiate with respect to the change $\blds{s}$
        \begin{equation}
            \nabla_s f(\blds{x}_k + \blds{s}) \approx \nabla f(\blds{x}_k) +
            \blds{H}\blds{s}
            \label{eq:secantEquationNewton}
        \end{equation}
    and introduce the condition in \Arf{eq:mincondition} and set this
    gradient to zero to find the change $\blds{s}$
        \begin{equation}
            \blds{s} = -\blds{H}^{-1} \nabla f(\blds{x}_k)
        \end{equation}
    Another way to determine this particular form for $\blds{s}$ is to say that
    the approximation to the Hessian must satisfy the \txtit{secant equation}
    which is \Arf{eq:secantEquationNewton}. The updating rule for $\blds{x}_n$
    is then given by
        \begin{equation}
            \blds{x}_{n+1} = \blds{x}_n - \gamma_k\blds{H}^{-1}_n\nabla
            f(\blds{x}_n)
        \end{equation}
    where $\gamma_k$ is again introduced to give some stability conditions. The
    important part of this equation is the index on the inverse Hessian. This
    is essentially just a relabeling at the change $\blds{s}$ is technically
    applied for each iterate $\blds{x}_n$. Note also that $\blds{s}$ takes the
    role of the search direction in this case. The algorithm is then to make an
    initial guess on the Hessian(usually just the identity matrix) and then use
    a type of updating formula that finds a new approximation for the Hessian
    at each step $n$. There are a number of these updating formulas, just to
    mention some we have DFP, SR1, McCormick, Broyden, BFGS and more. The one
    we will mention in more detail is the BFGS method, but a main formula that
    shows up in all of the updating methods is the \txtit{Sherman-Morrison
    formula} for the inverse. This basically means that the need for
    calculation of the inverse matrix is completely removed. \\

    With the mentioned expression we can devise an algorithm similar to
    Newton's method for finding the minimum $\blds{x}_m$. Starting with an
    initial guess for the inverse Hessian $\blds{H}^{-1}_0$ and minimum
    $\blds{x}_0$ with the condition that $\blds{H}^{-1}_0$ is positive-definite
    (identity matrix is a nice start if nothing else is known) proceed with
        \begin{itemize}
            \item $\blds{x}_{n+1} = \blds{x}_n -\gamma_n\blds{H}^{-1}_0\nabla
                f(\blds{x}_n)$
            \item Calculate the gradient $\nabla f(\blds{x}_{n+1})$
            \item Use $\nabla f(\blds{x}_{n+1})$ and $f(\blds{x}_n)$ in a
                updating formula of choice to find $\blds{H}^{-1}_{n+1}$
        \end{itemize}

\section{BFGS Method\label{sec:BFGS}}
    In the previous section we gave an outline for Newton's method and the
    class known as Quasi-Newton methods. The latter used an approximation for
    the inverse of the Hessian matrix, which was updated at each step in the
    algorithm. For the sake of brevity only conditions employed to arrive at
    the expression for the updating formula and the formula itself is given
    here, for more see \cite{BFGSB, BFGSF, BFGSG, BFGSS, numOptNocWrig}. The
    conditions enforced is
        \begin{itemize}
            \item Secant condition: $\blds{H}_{n+1} \blds{s}_n = \nabla
                f(\blds{x}_{n+1}) - \nabla f(\blds{x}_n)$
            \item Strong curvature: $\blds{s}^T_k\cdot (f(\blds{x}_{n+1}) -
                \nabla f(\blds{x}_n)) > 0$
        \end{itemize}
    and the resulting formula states with $\blds{y}_k = f(\blds{x}_{n+1}) -
    \nabla f(\blds{x}_n)$
        \begin{equation}
            \blds{H}_{n+1} = \blds{H}_n +
            \frac{\blds{y}_n\blds{y}^T_n}{\blds{y}^T_n \blds{s}_n} -
            \frac{\blds{H}_n\blds{s}_n\blds{s}^T_n\blds{H}_n}
            {\blds{s}^T_n\blds{H}_k\blds{s}_n}
            \label{eq:BFGSdef}
        \end{equation}
    With the Sherman-Morrison formula\cite{shermorInv} the inverse is updated
    with
        \begin{equation}
            \blds{H}^{-1}_{n+1} = \blds{H}^{-1}_n +
            \frac{\left(\blds{s}^T_n\blds{y}_n +
            \blds{y}^T_n\blds{H}^{-1}_n\blds{y}_n\right)
            \left(\blds{s}_n\blds{s}^T_n\right)}
            {\left(\blds{s}^T_n\blds{y}_n\right)^2} -
            \frac{\blds{H}^{-1}_n\blds{y}_n\blds{s}^T_n +
            \blds{s}_n\blds{y}^T_n\blds{H}^{-1}_n}{\blds{s}^T_n\blds{y}_n}
            \label{eq:shermanmorrisondef}
        \end{equation}

\section{Linesearch methods\label{sec:linesearch_methods}}
    In the optimization methods described in
    \Arf{sec:newtons-method_and_quasi-newton_methods} There was one important
    part neglected, namely how to find the step-length $\gamma$, introduced in
    the updating formula. As it is, one can choose it in any manner desired,
    however a class if one-dimensional minimization methods knows as
    \txtit{lineasearch methods} are often used to get an (usually rough)
    estimate for the step length at each iteration in the optimization. These
    methods all have some conditions for stability and convergence as an innate
    property, meaning the validity of the step length is better\footnote{It's
    actually present...}. Some popular linesearch methods are backtracking
    linesearch, Hager-Zhang method, Strong Wolfe conditions and the
    More-Thuente linesearch method. The one used here is the latter.  For an
    exact derivation and explanation of linesearch methods in general see
    \cite{numOptNocWrig}. See also the fantastic article by Jorge J.  Mor{\'e}
    and David J. Thuente\cite{moreThuenteArticle}. \\
    The basic idea of linesearch methods is to solve a one-dimensional problem
    of minimizing
        \begin{equation}
            \phi(\alpha) = f(\alpha \blds{p}_k + \blds{x}_k)
        \end{equation}
    with $f:\mathbb{R}^n\rarr\mathbb{R}$ and $\blds{p}_k$ is a search direction
    as described with the quasi-Newton methods and $\blds{x}_k$ is the current
    iterate(point) in the minimization. Notice also that
        \begin{equation}
            \prd{\alpha}[\phi] = \blds{p}_k \cdot \nabla f(\alpha \blds{p}_k +
            \blds{x}_k)
        \end{equation}
    by the chain-rule and the gradient on the right hand side is over the
    parameters $\blds{x}_k$. One usually perform this linesearch loosely since
    the search direction is not necessarily directly pointing towards the
    minimum, meaning we only search for a step length that gives a
    \txtit{sufficient decrease} in the function value $f$.

\section{Stochastic-Adaptive-BFGS\label{sec:stochastic_adaptive_bfgs}}
    In \Arf{sec:BFGS} we mentioned the popular BFGS method for updating the
    Hessian matrix and its inverse. A more resent method which uses that method
    with the stochastic nature of a functional expectation value is a method
    called SABFGS\cite{SABFGS} described by Zhou C., Gao W. and Goldfarb D.
    This method uses the BFGS update for the Hessian, but uses an adaptive step
    instead of the deterministic linesearch for the step-size. \\ 
    There one problem however, the method itself is only valid for
    \txtit{self-concordant} functions. The energy-functional is by-far not
    within this criteria. This problem can be accounted for by using the
    Wolfe-conditions. That is to check that the step-size satisfies the
    Wolfe-conditions at each iteration before actually making an update. The
    algorithm presented takes this into account aswell.

\section{Simulated Annealing\label{sec:simulated_annealing}}
    A huge problem with the mentioned methods is the fact that they only
    converge towards a \txtit{local minimum} which is not necessarily the
    global minimum. Many methods already exists to account for this, the one
    used in this thesis and to be described in this section is the method known
    as \txtit{simulated annealing}. Simulated annealing follows a simple
    algorithm, namely
        \begin{algorithm}[H]
            \caption{Simulated Annealing\label{alg:simulated_annealing}}
            \begin{algorithmic}[H]
                \State Initialize a solution $s=s_0$. \Comment{I.e a set of
                parameters $\{\alpha\}_{k=1}^N$}
                \For{$j=1$ to $M_{\text{max}}$}
                    \State Set temperature $T$ with specific function for
                    $\frac{j}{M_{\text{max}}}$
                    \State Pick a new state $s_{\text{new}}$ within some
                    neighbour of $s$
                    \If{$P(f(s), f(s_{\text{new}}), T) \geq \xi$} 
                    \Comment{$\xi$ uniformly distributed random in $[0,1]$.}
                        \State $s = s_{\text{new}}$
                    \EndIf
                \EndFor
            \end{algorithmic}
        \end{algorithm}
    The idea is to start with searching a large part of the solution space,
    since a high temperature increases the search-range, and hope that as $j$
    reaches $M_{\text{max}}$ the probability function $P$ is such that the
    solution is trapped within the down-hill of the global minimum. \\ 
    The specific form of $P, T$ and how to choose a neighbour is specific from
    problem to problem however an effective and simple way to define these is
    by using the Metropolis-algorithm with
        \begin{equation}
            P = \exp(-\frac{f({\text{new}) - f(s)}}{T})
        \end{equation}
    define the temperature as
        \begin{equation}
            T_j = \frac{j}{M_{\text{max}}}
        \end{equation}
    and choose new neighbours within some min/max range from the current $s$.
    >> REF THIS <<. \\
    After the annealing is done a Quasi-Newton method is used and then the
    adaptive method is used to converge to the minimum.
