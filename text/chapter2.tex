%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{}
    In this chapter we will address >> LIST METHODS << regarding computational
    quantum mechanics and further deepen into Hartree-Fock methods and
    Variational Monte Carlo method. Optimization of calculation is also given
    while structure of program is given in >> REF TO PROGRAM STRUCTURE CHAPTER
    <<. General statistical theory used is given in >> REF TO STATISTICS CHAPTER <<
\section{Quantum Monte Carlo}
    Quantum Monte Carlo, or QMC is a method for solving SchrÃ¶dinger's equation
    by a statistical approach using so-called \txtit{Markov Chain} simulations
    (also called random walk). The nature of the wave function at hand is
    fundamentally a statistical model defined on a large configuration space
    with small areas of densities. The Monte Carlo method is perfect for
    solving such a system because of the non-homogeneous distribution of
    calculation across the space. An standard approach with equal distribution
    of calculation would then yield a rather poor result with respect to
    computation cost. \\
    We will in this chapter address the Metropolis algorithm which is used to
    create a Markov chain and derive the equations used in the variational
    method. \\
    The chapter will use \txtit{Dirac Notation} \cite{GriffQuan} and all
    equations stated assume atom units ($\hbar=m_e=e=4\pi\veps_0$) >> REF HERE
    ATOMIC UNITS <<.

    \subsection{The Variational Principle and Expectation Value of Energy}
        Given a Hamiltonian $\Ham$ and a trial wave function $\psiT$, the
        variational principle \cite{GriffQuan, NeOr} states that the
        expectation value of $\Ham$
            \begin{equation}
                E[\psi_T] = \ecp{\Ham} =
                \frac{\dinner{\psi_T}{\Ham}}{\pinner}
                \label{eq:ecpE}
            \end{equation}
        is an upper bound to the ground state energy
            \begin{equation}
                E_0 \leq \ecp{\Ham}
                \label{eq:ecpEBound}
            \end{equation}
        Now we can define our PDF as
            \begin{equation}
                P(\mb{R}) \equiv \frac{\abs{\psi_T}^2}{\pinner}
                \label{eq:PDFdef}
            \end{equation}
        and with a new quantity
            \begin{equation}
                E_L(\mb{R};\mb{\alpha}) \equiv \frac{1}{\psiT}\Ham\psiT
                \label{eq:ELdef}
            \end{equation}
        the so-called local energy, we can rewrite \Arf{eq:ecpE} as
            \begin{equation}
                E[\psiT] = \ecp{E_L}
            \end{equation}

    \subsection{The Trial Wave Function}

    \subsection{Metropolis-Hastings Algorithm}
        The Metropolis algorithm bases itself on moves (also called
        transitions) as given in a Markov process. >> REF THIS HERE <<. This
        process is given by
            \begin{equation}
                w_i(t+\veps) = \sum_j\ufij{w}{i}{j}w_j(t)
            \end{equation}
        where $w(j\rarr i)$ is just a transition from state $j$ to state $i$.
        In order for the transition chain to reach a desired convergence while
        reversibility is kept, the well known condition for detailed balance
        must be fulfilled >> REF HERE DETAILED BALANCE <<. If detailed balance
        is true, then the following relations is true
            \begin{equation}
                w_i \ufij{T}{i}{j}\ufij{A}{i}{j} = w_j \ufij{T}{j}{i}\ufij{A}{j}{i}
                \Rarr \frac{w_i}{w_j} =
                \frac{\ufij{T}{j}{i}\ufij{A}{j}{i}}{\ufij{T}{i}{j}\ufij{A}{i}{j}}
                \label{eq:detailedBalance}
            \end{equation}
        We have here introduced two scenarios, the transition from
        configuration $i$ to configuration $j$ and the reverse process $j$ to
        $i$. Solving the acceptance $A$ for the two cases where the ratio in
        \ref{eq:detailedBalance} is either $1$(in which case the proposed state
        $j$ is accepted and transitions is made) and when the ratio is less
        then $1$. The Metropolis algorithm would in this case not automatically
        reject the latter case, but rather reject it with a proposed uniform
        probability. Introducing now a probability distribution function(PDF) $P$
        the acceptance $A$ can be expressed as
            \begin{equation}
                \ufij{A}{i}{j} =
                \text{min}\left(\frac{\ufij{P}{i}{j}}{\ufij{P}{j}{i}}
                \frac{\ufij{T}{i}{j}}{\ufij{T}{j}{i}} ,1\right)
                \label{eq:metropolisAcceptance}
            \end{equation}
        The so-called selection probability $T$ is defined specifically for
        each problem. For our case the PDF in question is the absolute square
        of the wave function and the selection $T$ is a Green's function given
        in >> REF GREENS <<. \\
        The algorithm itself would then be
            \begin{enumerate}[label=(\roman*)]
                \item Pick initial state $i$ at random.
                \item Pick proposed state at random in accordance to
                    $\ufij{T}{j}{i}$.
                \item Accept state according to $\ufij{A}{j}{i}$.
                \item Jump to step (ii) until a specified number of states have
                    been generated.
                \item Save the state $i$ and jump to step (ii).
            \end{enumerate}
