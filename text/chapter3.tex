%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Many-Body Quantum Theory\label{chapter:3}}     
    This chapter takes forth the theory regarding the basics of identical
    particles and \txtit{many-body quantum mechanics}. The reader is referred
    to \cite{GriffQuan} for an introductory text on quantum mechanics(for
    single particles) and also the so-called \txtit{Dirac-notation} used
    throughout the entire chapter. We will address >> LIST METHODS << regarding
    computational quantum mechanics and further deepen into Hartree-Fock
    methods and Variational Monte Carlo method. Optimization of calculation is
    also given while structure of program is given in >> REF TO PROGRAM
    STRUCTURE CHAPTER <<. General statistical theory used is given in >> REF TO
    STATISTICS CHAPTER <<

\section{The Hamiltonian and the Born-Oppenheimer Approximation\label{sec:3.1}}
    The task at hand is to solve the many-body system described by
    \txtit{Schrödinger's} equation
        \begin{equation}
            H\ket{\Psi_i} = E_i\ket{\Psi_i}
            \label{eq:SE}
        \end{equation}
    for some state $\ket{\Psi_i}$ with energy $E_i$. Usually the desired state
    is the ground-state energy $E_0$ of the system meaning we are primarily
    interested in the \txtit{ground-state} $\ket{\Psi_0}$. \\ With the goal
    determined we can define the system to consist of $N$ identical
    particles\footnote{These are in both atomic physics and in the quantum dot
    case always fermions or bosons.} with positions
    $\{\blds{r}_i\}^{N-1}_{i=0}$ and $A$ nuclei with positions
    $\{\blds{R}_k\}^{A-1}_{k=0}$. The Hamiltonian $H$ is then
        \begin{equation}
            H = - \frac{1}{2} \sum\limits_i \nabla^2_i + \sum_{i<j}
            f\left(\blds{r}_j, \blds{r}_j\right) - \frac{1}{2} \sum_k
            \frac{\nabla^2_k}{M_k} + \sum_{k<l}
            g\left(\blds{R}_k,\blds{R}_l\right) +
            V\left(\blds{R},\blds{r}\right)
        \end{equation}
    The first and second terms represent the kinetic- and inter-particle
    interaction terms\footnote{This is usually the well-known Coulomb
    interaction.} for the $N$ identical particles while the latter three
    represent kinetic- and interaction terms for the nuclei(with the last one
    being the nuclei-particle interaction). The constant $M_k$ is the mass of
    nucleon $k$ and $Z_k$ is the corresponding atomic number.

    We assume the nuclei to be much heavier than the identical particles,
    meaning they move much slower, at which the system can be viewed as
    electrons moving around the vicinity of stationary nuclei.  This means the
    kinetic term for the nuclei vanish and the nuclei-nuclei interaction
    becomes a constant\footnote{Adding a constant to an operator does not alter
    the eigenvector, only the eigenvalues by the constant
    factor\cite{linalgDavid}.}. The approximation we end up with is the
    so-called \txtit{Born-Oppenheimer approximation} and the Hamiltonian is now
        \begin{equation}
            H = H_0 + H_I
        \end{equation}
    where we have split the Hamiltonian in a \txtit{one-body} part and a
    \txtit{two-body} or \txtit{interaction} parts defined as
        \begin{equation}
            H_0 \equiv - \frac{1}{2} \sum\limits_i \nabla^2_i +
            V\left(\blds{R},\blds{r}\right)
        \end{equation}
    and
        \begin{equation}
            H_I \equiv \sum_{i<j} f\left(\blds{r}_i, \blds{r}_j\right)
        \end{equation}

\section{Slater Determinant and Permanent}
    Throughout \Arf{sec:3.1} we only referred to the wavefunction $\Psi$ as a
    state, a function closely connected to the probabilistic nature of the
    quantum particles. However, we have not given it a form. One possible
    solution is the \txtit{Hartree product} $\Psi_{\text{H}}$ defined as
        \begin{equation}
            \Psi_{\text{H}} = \prod_i\psi_i(\blds{r}_i)
        \end{equation}
    with $\{\psi\}_{i=0}^N$ being the orbitals which solve the single-particle
    Schrödinger equation for $H_0$. The Hartree-product is unfortunately a poor
    choice since it does not solve the $H_I$ part meaning it is not a
    physically valid solution. This comes from the fact that the
    Hartree-product does not take into account the fact that the particles in
    question are \txtit{identical particles}. Since the particles are
    identical, switching the labels on the particles shouldn't change the
    expectation value of some observable. If we run this remark through we
    end up with the conclusion that the state $\ket{\Psi}$ must be either
    symmetric or antisymmetric with the symmetric part being the \txtit{bosonic
    state} and antisymmetric being the \txtit{fermionic state}. The connection
    between antisymmetric states and fermions is called the \txtit{Pauli
    exclusion principle}.

    The problem with the Hartree-product is, with the above sentiment, that it
    is not symmetric nor antisymmetric. However we can transform it with an
    operator
        \begin{equation}
            \mathcal{B} \equiv \frac{1}{N!}\sum_P\sigma_bP
        \end{equation}
    where $\sigma_b$ is defined as 
        \begin{equation}
            \sigma_b \equiv  
                \begin{cases}
                    1 \indent &\text{$b$ represents bosonic system} \\
                    (-1)^p \indent &\text{$b$ represents fermionic system}
                \end{cases}
        \end{equation}
    $P$ is a permutation operator that switches the labels on particles
    \footnote{$P_{ij}\Psi_(\blds{r}_1,\dots,\blds{r}_i,\dots,\blds{r}_j,\dot,\blds{r}_N)
    = \Psi_(\blds{r}_1,\dots,\blds{r}_j,\dots,\blds{r}_i,\dot,\blds{r}_N)$
    \cite{compphysThijssen}.} and $p$ is the parity of permutations. The
    operator $\mathcal{B}$ has the following properties
        \begin{itemize}
            \item Applying $\mathcal{B}$ to itself doesn't change the operator
                meaning $\mathcal{B}^2 = \mathcal{B}$.
            \item The Hamiltonian $H$ and $\mathcal{B}$ \txtit{commute}, that
                is $\left[\mathcal{B},H\right] = \left[H,\mathcal{B}\right]$.
            \item $\mathcal{B}$ is \txtit{unitary}, which means
                $\mathcal{B}^{\dagger}\mathcal{B}=\mathcal{I}$.
        \end{itemize}
    The solution $\Psi_T$ to the Schrödinger equation can now be written as
        \begin{equation}
            \Psi_T\left(\blds{r}\right) =
            \sqrt{N!}\mathcal{B}\Psi_{\text{H}}\left(\blds{r}\right)
            \label{eq:psiOpB}
        \end{equation}
    The antisymmetric case of $\mathcal{B}$ results in a \txtit{Slater
    determinant} 
        \begin{equation}
            \Psi^{\text{AS}}_T = \frac{1}{\sqrt{N!}}\sum_{P}(-1)^pP\prod_i\psi_i
        \end{equation}
    while the symmetric case gives the so-called \txtit{permanent}\footnote{The
    permanent is basically just a determinant with all the negative signs
    replaced by positive ones.}.
        \begin{equation}
            \Psi^{\text{S}}_T = \frac{1}{\sqrt{N!}}\sum_{P}P\prod_i\psi_i
        \end{equation}
    Notice that the coordinated ${\blds{r}}_{i=0}^{N}$ is a bit of a sloppy
    notation as it also implicitly includes the spin orbitals discussed in >>
    ref section on spin orbitals <<.

\section{Variational Principle\label{sec:varPrinc}}
    One important remark is that the Slater determinant and the permanent do
    not solve the interaction part, but only serves as a so-called
    \txtit{ansatz} or guess on the true ground-state wavefunction. This is
    quite useful due to the \txtit{variational principle}. \\
    The Variational principle states that for any normalized function $\Psi$ in
    Hilbert Space >> REF HILBERT << with a Hermitian operator $H$ the minimum
    eigenvalue $E_0$ for $H$ has an upper-bound given by the expectation value
    of $H$ in the function $\Psi$. That is
        \begin{equation}
            E_0 \leq \ecp{H} = \bra{\Psi}H\ket{\Psi} = \int \Psi^{*}H\Psi \md r
            \label{eq:varPrinc}
        \end{equation}
    See \cite{GriffQuan} for proof and more.

    The mentioned ansatz is thereby guaranteed to give energies larger than or
    equal the true ground state energy meaning a minimization method is
    sufficient in order to get closer to this minimum. 
   
\section{Energy Functional\label{sec:energyFunc}}
    We can find a more convenient expression for this energy by using
    \Arf{eq:psiOpB, eq:varPrinc}. This gives us
        \begin{equation}
            E\left[\Psi\right] =
            N!\bra{\Psi_{\text{H}}}H\mathcal{B}\ket{\Psi_{\text{H}}}
        \end{equation}
    where the hermitian and unitary property of $\mathcal{B}$ as well as the
    fact that $\mathcal{B}$ and $H$ commute have been used. This energy
    functional(functional in the sense that it is dependant on the wave
    function). Applying the $\mathcal{B}$ operator to the Hartree-product,
    pulling the sum out of the integrals and relabeling with the following
    definitions
        \begin{equation}
            \begin{aligned}
                \bra{p}h\ket{q} &\equiv
                \bra{\psi_p(\blds{r})}h(\blds{r})\ket{\psi_q(\blds{r})} =
                \int\psi^{*}_p(x)h(\blds{r})\psi_q(\blds{r})\md r \\
                \bra{pq}f\ket{rs} &\equiv
                \bra{\psi_p(\blds{r}_1)\psi_q(\blds{r}_2)}
                f(\blds{r}_1,\blds{r}_2)
                \ket{\psi_r(\blds{r}_1)\psi_s(\blds{r}_2)} = \int
                \psi_p(\blds{r}_1)\psi_q(\blds{r}_2) f(\blds{r}_1,\blds{r}_2)
                \psi_r(\blds{r}_1)\psi_s(\blds{r}_2) \md r
            \end{aligned}
        \end{equation}
    yields in
        \begin{equation}
            E\left[\Psi\right] = \bra{p}H_0\ket{p} +
            \frac{1}{2}\sum_{p,q}\left[\bra{pq}f_{pq}\ket{pq} \pm
            \bra{pq}f_{pq}\ket{qp}\right]
            \label{eq:ch3Efunc}
        \end{equation}
    The first part is written with the assumption that the single-particle wave
    functions $\{\psi\}$ are orthogonal and the $1/2$ factor in front of the
    so-called \txtit{direct} and \txtit{exchange} terms\footnote{The direct
    term is just due to inherent behaviour of the charge of the particles
    (known as the Coulomb repulsion). The exchange term is a direct consequence
    of the probabilistic nature of the identical particles.} is due to the fact
    that we count the permutations twice in the sum when applying the
    $\mathcal{B}$ operator. The sign in the interaction term are chosen as
    positive for bosonic systems and negative for fermionic systems.

    The expression given in \Arf{eq:ch3Efunc} is the functional form we will
    use to derive the Hartree-fock equations in the following section.

\section{Hartree-Fock Theory\label{sec:HFtheory}}
    Hartree-Fock method is a many-body method for approximating the
    wavefunction of a stationary many-body quantum state and thereby also
    obtain an estimate for the energy in this state. In this section we will
    derive the Hartree-Fock equations from scratch, following closely the
    literature by J.M Thjissen\cite{compphysThijssen}. 

    \subsection{Assumptions}
        Hartree-Fock method makes the following assumptions of the system
            \begin{itemize}
                \item \txtit{The Born-Oppenheimer approximation}, see >> REF BO <<.
                \item All relativistic effects are negligible.
                \item The wavefunction can be described by a single
                    \txtit{Slater determinant}.
                \item The \txtit{Mean Field Approximation} holds.
            \end{itemize}
        With these inherent approximations the last one is the most important
        to take into account as it can cause large deviations from test
        solutions (analytic solutions, experimental data etc.) since the
        electron correlations is in reality, for many cases, not negligible.
        There exists many methods that try to fix this problem >> LIST METHODS
        <<. The \txtit{Variational Monte Carlo} (or VMC) is the method for
        deeper explorations in this Thesis, see \Arf{sec:QMC} for more details.

    \subsection{The $\blds{\mathcal{J}}$ and $\blds{\mathcal{K}}$ Operators}
        Before we begin with the Hartree-Fock equations it is desirable to
        rewrite the energy function obtained in \Arf{sec:energyFunc} (form
        given in \Arf{eq:ch3Efunc}) with two operators $\mathcal{J}$ and
        $\mathcal{K}$ defined as
            \begin{equation}
                \begin{aligned}
                    \mathcal{J} &\equiv \sum_k
                    \bra{\psi^{*}_k}f_{12}\ket{\psi_k} = \int
                    \psi^{*}_k(\blds{r})f_{12}\psi_k(\blds{r})\md r \\
                    \mathcal{K} &\equiv \sum_k
                    \bra{\psi^{*}_k}f_{12}\ket{\psi} = \int
                    \psi^{*}_k(\blds{r})f_{12}\psi(\blds{r})\md r \\
                \end{aligned}
            \end{equation}
        The $\mathcal{J}$ operator just gives the simple interaction-term while
        the $\mathcal{K}$ operator gives the exchange term with the arbitrary
        (notice no index) $\psi(\blds{r})$. The energy functional is thus
        rewritten to
            \begin{equation}
                E\left[\Psi\right] = \sum_i\Braket{\psi_i | h +
                \frac{1}{2}\left(\mathcal{J} \pm \mathcal{K}\right) | \psi_i}
                \label{eq:JKEfunc}
            \end{equation}
        where the one-body Hamiltonian is split into a sum of single particle
        functions as $H_0 = \sum\limits_i h(\blds{r}_i)$.

    \subsection{Hartree-Fock Equations}
        As a reminder. The wavefunctions $\{\psi\}$ in \Arf{eq:JKEfunc} are
        spin-orbitals with both a spacial part and a spin part. In order to
        obtain the Hartree-Fock equations we try to minimize the energy
        functional in order to obtain the ground-state energy for a many-body
        system. This is done by a variational method.

        The first observation to notice is the fact that variations in the
        spin-orbitals $\{\psi\}$ need to respect the spin-orthogonality
        relation
            \begin{equation}
                \braket{\psi_i}{\psi_j} = \delta_{ij}
            \end{equation}
        with $\delta_{ij}$ being the well-known Kronecker-delta. This property
        is essentially a constraint to the minimization problem and the method
        to be used is the \txtit{Lagrange multiplier method}\cite{linalgDavid},
        with the following \txtit{Lagrangian}
            \begin{equation}
                \mathcal{L} = \delta E\left[\Psi\right] -
                \sum_{ij}\Lambda_{ij}\left[\braket{\psi_i}{\psi_j} -
                \delta_{ij}\right]
            \end{equation}
        We know then that the minimum is reached when a displacement on the
        spin-orbitals $\psi_i\rarr\psi_i+\delta \psi_i$ results in an energy
        variation of zero meaning $\delta E\left[\Psi\right]=0$ in the minimum.
        Which giving the variational problem 
            \begin{equation}
                \sum_i \Braket{\delta\psi_i | h + \mathcal{J} \pm \mathcal{K} |
                \psi_i} - \sum_{ij} \Lambda_{ij}\braket{\delta\psi_i}{\psi_j} +
                \text{c.c} = 0
            \end{equation}
        where $\text{c.c}$ is a notation for the complex conjugate of the
        inner-products on its left-hand side.

        The shift in the spin orbitals $\{\delta \psi\}$ is arbitrary and the
        constraints are
        symmetric\footnote{$\braket{\psi_i}{\psi_j}=\braket{\psi_j}{\psi_i}^{*}
        \Rarr \Lambda_{ij}=\Lambda^{*}_{ji}$} meaning we can with the
        \txtit{Fock-operator}
            \begin{equation}
                \mathcal{F} \equiv h + \mathcal{J} \pm \mathcal{K}
            \end{equation}
        define the following eigenvalue problem
            \begin{equation}
                \mathcal{F}\psi_i = \sum_j\Lambda_{ij}\psi_j
            \end{equation}
        Choosing the Lagrange parameter $\Lambda_{ij}$ such that
        $\{\psi\}^N_{k=1}$ forms an orthonormal set for $\mathcal{F}$ with
        eigenvalues $\{\veps\}^N_{k=1}$. This reduces the eigenvalue equation
        to
            \begin{equation}
                \mathcal{F}\ket{\psi} = \blds{\veps}\ket{\psi}
                \label{eq:HFequations}
            \end{equation}
        with $\blds{\veps}=(\veps_0,\dots,\veps_N)$ being the set of
        eigenvalues of $\mathcal{F}$ meaning we have $N+1$ equations to be
        solved.

        If we only take the $N$ lowest eigenfunctions into the Slater the
        corresponding eigenenergy is referred to as the \txtit{Hartree-Fock
        energy} and is the estimated ground-state energy which the Hartree-Fock
        method gives. We can rewrite the energy functional with the
        eigenenergies to
            \begin{equation}
                E\left[\Psi\right] = \sum_i\Braket{\psi_i | \veps_i -
                \frac{1}{2}\left(\mathcal{J} \pm \mathcal{K}\right) | \psi_i}
            \end{equation}
        In the derivation of the Hartree-Fock equations we only worked with
        spin-orbital functions $\{\psi\}$. However it is much more convenient
        to rewrite these in terms of spatial orbitals $\{\phi\}$ and integrate
        the spin-dependant part out. There are two ways of doing this and the
        two different approaches give the so-called \txtit{restricted
        Hartree-Fock} and \txtit{unrestricted Hartree-Fock} methods.

    \section{Restricted Hartree-Fock and Roothan-Equations}
        The restricted spin-orbitals are paired as\footnote{This is specialised
        for a two-spin system. For a system with more spin-states one needs to
        either choose different spacial-orbitals or add more such orbitals
        which effectively changes the energy-levels.}
            \begin{equation}
                \{\psi_{2l-1}, \psi_{2l}\} =
                \{\phi_l(\blds{r})\alpha(s),\phi_l(\blds{r})\beta(s)\}
            \end{equation}
        with $\alpha(s)$ and $\beta(s)$ being different spin-states (up and
        down). This pairing of spin-states with same and same spacial-orbitals
        means we can pull the spin degrees of freedom out from the
        $\mathcal{J}$ and $\mathcal{K}$ operators, reduce the sum to only run
        over half the states and multiply the entire sum by $2$. The result is
        that the restricted energy-functional reads
            \begin{equation}
                E\left[\Psi\right] = \sum\limits^N_{i=1}\veps_i -
                \sum\limits^{\frac{N}{2}}_{i=1} \Braket{i | 2\mathcal{J}
                \pm \mathcal{K} | i}
            \end{equation}
        Notice that the $\mathcal{K}$ operators sum only runs up to half the
        number of states.
        
        As the title suggests we are going to end up with a set of equations
        referred to as the \txtit{Roothan-equations}. We start by first
        expanding the spacial part $\{\phi\}$ of the spin orbitals $\{\psi\}$
        in some known orthonormal basis $\{\chi\}^L_{i=1}$
            \begin{equation}
                \phi_i(\blds{r}) = \sum\limits^L_{p=1} C_{pi}\chi_p(\blds{r})
            \end{equation}
        and introduce the \txtit{Fock-matrix} $F$(associated with the
        Fock-operator) with elements
            \begin{equation}
                F_{pq} = h_{pq} + \sum_{pq}\rho_{pq}\left(2D_{prqs} \pm
                D_{prsq}\right)
            \end{equation}
        We have here introduced a one-body matrix defined as
            \begin{equation}
                h_{pq} \equiv \Braket{p | h | q}
            \end{equation}
        a \txtit{density matrix} defined
        as\footnote{This is just the matrix formed by \begin{equation}
        \sum_i\ket{\phi_i}\bra{\psi_i}\end{equation} which is in quantum
        mechanics defined as the so-called \txtit{density matrix}.}
            \begin{equation}
                \rho_{pq} \equiv \sum\limits^{\frac{N}{2}}_{i=1}
                C_{pi}C^{*}_{qi}
            \end{equation}
        and an interaction-matrix $D$ with elements
            \begin{equation}
                D_{pqrs} \equiv \Braket{pq | f_{12} | rs}
            \end{equation}
        for convenience. The implicit relabeling of $\chi_p(\blds{r})\rarr p$
        is also present in the above expression for the Fock-matrix. The
        Hartree-Fock equations (\Arf{eq:HFequations}) are then for the
        restricted case written as
            \begin{equation}
                F\blds{C}_i = \blds{\veps}S\blds{C}_i
            \end{equation}
        with $S$ being the overlap matrix with elements
            \begin{equation}
                S_{pq} \equiv \Braket{p | q}
            \end{equation}

\section{Quantum Monte Carlo\label{sec:QMC}}
    Quantum Monte Carlo, or QMC is a method for solving Schrödinger's equation
    by a statistical approach using so-called \txtit{Markov Chain} simulations
    (also called random walk). The nature of the wave function at hand is
    fundamentally a statistical model defined on a large configuration space
    with small areas of densities. The Monte Carlo method is perfect for
    solving such a system because of the non-homogeneous distribution of
    calculation across the space. An standard approach with equal distribution
    of calculation would then yield a rather poor result with respect to
    computation cost.

    We will in this chapter address the Metropolis algorithm which is used to
    create a Markov chain and derive the equations used in the variational
    method.

    The chapter will use \txtit{Dirac Notation} \cite{GriffQuan} and all
    equations stated assume atomic units ($\hbar=m_e=e=4\pi\veps_0$) >> REF
    HERE ATOMIC UNITS <<.

    \subsection{The Variational Principle and Expectation Value of Energy}
        Given a Hamiltonian $\Ham$ and a trial wave function $\psiT$, the
        variational principle \cite{GriffQuan, NeOr} states that the
        expectation value of $\Ham$
            \begin{equation}
                E[\psi_T] = \ecp{\Ham} =
                \frac{\dinner{\psi_T}{\Ham}}{\pinner}
                \label{eq:ecpE}
            \end{equation}
        is an upper bound to the ground state energy
            \begin{equation}
                E_0 \leq \ecp{\Ham}
                \label{eq:ecpEBound}
            \end{equation}
        Now we can define our PDF as(see \Arf{susec:diffTHpdf} for a more
        detailed reasoning)
            \begin{equation}
                P(\mb{R}) \equiv \frac{\abs{\psi_T}^2}{\pinner}
                \label{eq:PDFdef}
            \end{equation}
        and with a new quantity
            \begin{equation}
                E_L(\mb{R};\mb{\alpha}) \equiv \frac{1}{\psiT}\Ham\psiT
                \label{eq:ELdef}
            \end{equation}
        the so-called local energy, we can rewrite \Arf{eq:ecpE} as
            \begin{equation}
                E[\psiT] = \ecp{E_L}
            \end{equation}
        The idea now is to find the lowest possible energy by varying a set of
        parameters $\mb{\alpha}$. The expectation value itself is found with
        the Metropolis algorithm, see \Arf{susec:MHAlg}. \\

        An important property of the local energy is when we differentiate it
        with respect to one of the variational parameters $\{\alpha\}$ within
        the context of an expectation value. The result in this case would be
        zero. This is easily seen by direct calculation
            \begin{align}
                \ecp{\prd{\alpha}[E_L]} &= \int \frac{\abs{\psi}^2\prd{\alpha}
                \left[\frac{1}{\psi}H\psi\right]}{\int \abs{\psi}^2 \md r} \md
                r \nonumber \\
                &= \int \frac{\abs{\psi}^2 \frac{\psi^{*}\prd{\alpha}(H\psi) -
                (H\psi^{*}) \prd{\alpha}[\psi]}{\abs{\psi}^2}}{\int
                \abs{\psi}^2 \md r} \md r \nonumber \\
                &= \int \frac{\psi^{*}H\prd{\alpha}[\psi] -
                \psi^{*}H\prd{\alpha}[\psi]}{\int \abs{\psi}^2 \md r} \md r
                \nonumber \\
                &= 0
                \label{eq:expELdelzero}
            \end{align}
        We have used the fact that $H$ is not dependant on any variational
        parameter and used the hermitian properties\cite{GriffQuan} of $H$ to
        justify the movement of $H$ within the integral. 

        This neat result presented in \Arf{eq:expELdelzero} will show its
        usefulness in the minimization when derivatives of the expectation
        value and variance comes into play since finding the derivative of the
        local energy would be much more of a hassle.\footnote{The
        differentiation of the wavefunction is enough reduction of quality of
        life on its own!}

    \subsection{The Trial Wave Function}
        The trial wave function is generally an arbitrary choice specific for
        the problem at hand, however it is in most cases favorable to expand
        the wave function in the eigenbasis (eigenstates) of the Hamiltonian
        since they forma complete set. This can be expressed as
            \begin{equation}
                \psiT = \sum_i C_i\psi_i(\mb{R};\mb{\alpha})
            \end{equation}
        with the $\psi_i$'s are the eigenstates of the Hamiltonian.

    \subsection{Use Diffusion Theory and the PDF\label{susec:diffTHpdf}}
        The statistics describing the expectation value states that any
        distribution may be applied in calculation, however if we take a close
        look at the local energy(\Arf{eq:ELdef}) we see that for all
        distributions the local energy is not defined at the zeros of $\psiT$.
        This means that an arbitrary PDF does not guarantee generation of
        points which makes $\psi_T=0$. This can be overcome by introducing the
        square of the wave function to be defined as the distribution function
        as given in \Arf{eq:PDFdef}.

        Because of the inherent statistical property of the wave function
        Quantum Mechanics can be modelled as a diffusion process, or more
        specifically, an \txtit{Isotropic Diffusion Process} which is
        essentially just a random walk model. Such a process is described by
        the Langevin equation with the corresponding Fokker-Planck equation
        describing the motion of the walkers(particles). See \cite{numstoch}
        for details.

    \subsection{Metropolis-Hastings Algorithm\label{susec:MHAlg}}
        The Metropolis algorithm bases itself on moves (also called
        transitions) as given in a Markov process. >> REF THIS HERE <<. This
        process is given by
            \begin{equation}
                w_i(t+\veps) = \sum_j\ufij{w}{i}{j}w_j(t)
            \end{equation}
        where $w(j\rarr i)$ is just a transition from state $j$ to state $i$.
        In order for the transition chain to reach a desired convergence while
        reversibility is kept, the well known condition for detailed balance
        must be fulfilled >> REF HERE DETAILED BALANCE <<. If detailed balance
        is true, then the following relations is true
            \begin{equation}
                w_i \ufij{T}{i}{j}\ufij{A}{i}{j} = w_j \ufij{T}{j}{i}\ufij{A}{j}{i}
                \Rarr \frac{w_i}{w_j} =
                \frac{\ufij{T}{j}{i}\ufij{A}{j}{i}}{\ufij{T}{i}{j}\ufij{A}{i}{j}}
                \label{eq:detailedBalance}
            \end{equation}
        We have here introduced two scenarios, the transition from
        configuration $i$ to configuration $j$ and the reverse process $j$ to
        $i$. Solving the acceptance $A$ for the two cases where the ratio in
        \ref{eq:detailedBalance} is either $1$(in which case the proposed state
        $j$ is accepted and transitions is made) and when the ratio is less
        then $1$. The Metropolis algorithm would in this case not automatically
        reject the latter case, but rather reject it with a proposed uniform
        probability. Introducing now a probability distribution function(PDF) $P$
        the acceptance $A$ can be expressed as
            \begin{equation}
                \ufij{A}{i}{j} =
                \text{min}\left(\frac{\ufij{P}{i}{j}}{\ufij{P}{j}{i}}
                \frac{\ufij{T}{i}{j}}{\ufij{T}{j}{i}} ,1\right)
                \label{eq:metropolisAcceptance}
            \end{equation}
        The so-called selection probability $T$ is defined specifically for
        each problem. For our case the PDF in question is the absolute square
        of the wave function and the selection $T$ is a Green's function
        derived in \Arf{susec:impSamp}.  The algorithm itself would then be
            \begin{enumerate}[label=(\roman*)]
                \item Pick initial state $i$ at random.
                \item Pick proposed state at random in accordance to
                    $\ufij{T}{j}{i}$.
                \item Accept state according to $\ufij{A}{j}{i}$.
                \item Jump to step (ii) until a specified number of states have
                    been generated.
                \item Save the state $i$ and jump to step (ii).
            \end{enumerate}

    \subsection{Importance Sampling\label{susec:impSamp}}
        Using the selection probability mentioned in \Arf{susec:MHAlg} in the
        Metropolis algorithm is called an \txtit{Importance sampling} because
        is essentially makes the sampling more concentrated around areas where
        the PDF has large values.

        In order to derive the form of this equation we use the statements
        presented in \Arf{susec:diffTHpdf}. With
            \begin{equation}
                \langevin
                \label{eq:langevin}
            \end{equation}
        the \txtit{Langevin equation} >>REF HERE LANGEVIN<< and apply Euler's
        method (Euler-Maryama >>REF<<) and obtain the new positions
            \begin{equation}
                \rnew = \rold + D\Fold\Delta t + \xi
                \label{eq:rnewdef}
            \end{equation}
        with the $r$'s being the new and old positions in the Markov chain
        respectively and $\Fold=F(\rold)$. The quantity $D$ is a diffusion
        therm equal to $1/2$ due to the kinetic energy(remind of natural units)
        and $\xi$ is a Gaussian distributed random number with $0$ mean and
        $\sqrt{\Delta t}$ variance.

        As mentioned a particle is described by the Fokker-Planck equation
            \begin{equation}
                \FokkerPlanck
                \label{eq:FokkerPlanckDef}
            \end{equation}
        With $P$ being the PDF(in current case the selection probability) and
        $F$ being the drift therm. In order to achieve convergence, that is a
        stationary probability density, we need the left hand side to be zero
        in \Arf{eq:FokkerPlanckDef} giving the following equation
            \begin{equation}
                \prd{x_i}[P][2] = P\prd{x_i}[\mb{F_i}] + \mb{F_i}\prd{x_i}[P]
            \end{equation}
        with the drift-therm being on the form $\mb{F}=g(x)\prtl P/\prtl x$ we
        finally have that
            \begin{equation}
                \mb{F} = \frac{2}{\psi_T}\nabla \psi_T
                \label{eq:qForceDef}
            \end{equation}
        This is the so-called \txtit{Quantum Force} which pushes the walkers
        towards regions where the wave function is large.
