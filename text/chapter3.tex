%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\label{chapter:3}}
    In this chapter we will address >> LIST METHODS << regarding computational
    quantum mechanics and further deepen into Hartree-Fock methods and
    Variational Monte Carlo method. Optimization of calculation is also given
    while structure of program is given in >> REF TO PROGRAM STRUCTURE CHAPTER
    <<. General statistical theory used is given in >> REF TO STATISTICS CHAPTER <<

\section{Hartree-Fock Theory}
    Hartree-Fock theory method is a method for approximating the wavefunction
    of a stationary many-body quantum state and thereby also obtain an estimate
    for the energy in this state. In this section we will derive the
    Hartree-Fock equations from scratch, following closely >> REF SOMETHING
    HERE << by >> REF AUTHOR << and find the so-called Roothaan-Hall equations
    known as \txtit{Unrestricted Hartree-Fock} method, which is also the method
    used for obtaining the results given in >> REF RESULTS CHAPTER <<.

    \subsection{Assumptions}
        Hartree-Fock method makes the following assumptions of the system
            \begin{itemize}
                \item \txtit{The Born-Oppenheimer approximation}, see >> REF BO <<.
                \item All relativistic effects are negligible.
                \item The wavefunction can be described by a single Slater
                    determinant >> REF SLATER << or permanent in case of
                    bosons(former is for fermions).
                \item The \txtit{Mean Field Approximation} holds.
            \end{itemize}
        With these inherent approximations the last one is the most important
        to take into account as it can cause large deviations from test
        solutions (analytic solutions, experimental data etc.) since the
        electron correlations is in reality, for many cases, not negligible.
        There exists many methods that try to fix this problem >> LIST METHODS
        <<. The \txtit{Variational Monte Carlo} (or VMC) is the method for
        deeper explorations in this Thesis, see \Arf{sec:QMC} for more details.

    \subsection{Energy Functional}
        The general expression (for a general system) for the energy is given
        in \Arf{sec:varPrinc}, we restate it
            \begin{equation}
                E[\psiT] = \frac{\bra{\psiT}H\ket{\psiT}}{\braket{\psiT}}
                \label{eq:energyFunc}
            \end{equation}
        The denominator is just the normalization factor.

        Before we can obtain the Hartree-Fock equations we need an expression
        for the expectation value of the energy, an \txtit{energy functional}
        (functional in the sense that it is dependant on the wavefunction).

        With the mentioned Born-Oppenheimer approximation we set up the
        SchrÃ¶dinger equation >> REF SL << with the following Hamiltonian >> REF
        HAMILTONIAN <<
            \begin{equation}
                H \equiv H_0 + H_I
                \label{eq:hamiltoninandef}
            \end{equation}
        where $H_0$ is the Hamiltonian of some a system with analytic solutions
        to the wavefunction(i.e harmonic oscillator) meaning
            \begin{equation}
                H_0 = \HO
            \end{equation}
        where $\blds{R}=\blds{r}_1,\dots,\blds{r}_N$ (the positions of the
        particles) and $V$ is the expression for the potential of the system.
        The second part of \Arf{eq:hamiltoninandef} will be referred to as the
        \txtit{Interaction Hamiltonian} and is assumed to be a function of the
        inter-particle distances $\blds{r}_i-\blds{r}_j$ meaning
            \begin{equation}
                H_I = \HI
            \end{equation}
        with $f$ being the function describing the interaction between two
        particles labeled $i$ and $j$ (for instance the (Coloumb interation due
        to the charge of particles).

        As a summary and later reference the full Hamiltonian of the system is
            \begin{equation}
                H = \HO + \HI
            \end{equation}
        which, when inserted into \Arf{eq:energyFunc}, gives
            \begin{equation}
                E[\Psi_T] = \bra{\Psi_T}V\ket{\Psi_T} -
                \frac{1}{2}\sum_i\bra{\Psi_T}\nabla^2_i\ket{\Psi_T} +
                \sum\limits_{i<j}\bra{\Psi_T}f\ket{\Psi_T}
            \end{equation}
        where we have omitted the normalization factor and argument variables.
        We start with the part involving $H_0$ giving
            \begin{align}
                \dinner{\Psi_T}{H_0} &= \sum_{i}\dinner{\Psi_T}{h_i} \nonumber
                \\
                &= \sum_i\dinner{\psi_{ii}}{h_i} 
            \end{align}
        with $h_i$ being the single-particle Hamiltonian (for a particle $i$).
        The final result here is due to the fact that $h_i$ only acts on
        particle $i$ and that the $\psi$'s are orthogonal.

        Before we go any further with $H_I$ let us introduce the so-called
        permutation operator $P$ which interchanges the labels of particles
        meaning we can define
            \begin{equation}
                A \equiv \frac{1}{N!}\sum_p(-1)^pP
            \end{equation}
        the so-called \txtit{antisymmetrization} operator. This operator has
        the following traits
            \begin{itemize}
                \item The Hamiltonian $H$ and $A$ commute since the Hamiltonian
                    is invariant under permutation.
                \item $A$ applied on itself (that is $A^2$) is equal to itself
                    since permuting a permuted state reproduces the state.
            \end{itemize}
        We can now express out Slater $\Psi_T$ in terms of $A$ as
            \begin{equation}
                \Psi_T = \sqrt{N!}A\prod_{i,j}\psi_{ij}
            \end{equation}
        where $\psi_{ij}=\psi_j(\blds{r}_i)$ is element $i,j$ of the Slater
        matrix (the matrix associated with the Slater determinant $\Psi_T$).

        The interaction part of $H$ is then
            \begin{equation}
                \dinner{\Psi_T}{H_I} = N!\prod_{i,j}\dinner{\psi_{ij}}{AH_IA}
            \end{equation}
        The interaction $H_I$ and $A$ commute since $A$ commutes with $H$
        giving
            \begin{align}
                AH_IA\ket{\psi_{ij}} &= \frac{1}{N!^2}\sum_{i<j}\sum_{p}
                (-1)^{2p}\fij P\ket{\psi_{ij}} \\
                &= \frac{1}{N!^2}\sum_{i<j}\fij(1-P_{ij})\ket{\psi_{ij}}
            \end{align}
        The factor $1-P_{ij}$ comes from the fact that contributions with
        $i\neq j$ vanishes due to orthogonality when $P$ is applied. The final
        expression for the interaction term is thus
            \begin{equation}
                \dinner{\Psi_T}{H_I} =
                \sum_{i<j}\prod_{k,l}\left[\dinner{\psi_{kl}}{\fij} -
                \bra{\psi_{kl}}\fij\ket{\psi_{lk}}\right]
            \end{equation}
        Writing out the product and realizing the double summation over pairs
        of states we end up with
            \begin{equation}
                \dinner{\Psi_T}{H_I} = \frac{1}{2}
                \sum_{i,j}\left[\dinner{\psi_{ij}\psi_{ji}}{\fij} -
                \bra{\psi_{ij}\psi_{ji}}\fij\ket{\psi_{ji}\psi_{ij}}\right]
            \end{equation}
        Relabeling the indices in terms of just one index $p$ and $q$ to
        represent just a state (the integration label does not matter) we get
        the following energy functional
            \begin{equation}
                E[\Psi_T] = \sum_p\bra{\psi_p}h\ket{\psi_p} +
                \frac{1}{2}\sum_{p,q}
                \bra{\psi_p\psi_q}H_I\ket{\psi_p\psi_q}_{AS}
                \label{eq:EfuncFinal}
            \end{equation}
        We drop the index in $h$ since the integration label does not matter.
        We have also introduced the notation
            \begin{equation}
                \bra{pq}v\ket{rs}_{AS} = \bra{pq}v\ket{rs} - \bra{pq}v\ket{sr}
            \end{equation}
        referred to as the \txtit{antisymmetric part} with $v$ being some
        hermitian operator.

        The expression given in \Arf{eq:EfuncFinal} is the functional form in
        its general form (since we haven't given any special form for the basis
        spanned by the $\psi$'s). In the next section we will derive the
        Hartree-Fock equations using \Arf{eq:EfuncFinal}.

    \subsection{Hartree-Fock Equations}
        In order to obtain the Hartree-Fock equations we start by expanding the
        wavefunction as a linear combination of some \txtit{known orthonormal
        basis} $\{\phi_{\lambda}(\blds{r})\}$ meaning
            \begin{equation}
                \Psi^{\text{HF}}_p = \sum_{\lambda}
                C_{p\lambda}\phi_{\lambda}(\blds{r})
                \label{eq:psiexpand}
            \end{equation}
        This expanded basis is obviously also orthonormal. The energy
        functional (\Arf{eq:EfuncFinal}) is then with \Arf{eq:psiexpand}
        inserted
            \begin{equation}
                \EHF = \sum_i\sum_{\alpha\beta}
                C^*_{i\alpha}C_{j\beta}\bra{\alpha}h\ket{\beta} + \frac{1}{2}
                \sum_{ij} \sum_{\alpha\beta\gamma\delta}
                C^{*}_{i\alpha}C^{*}_{j\beta}C_{i\gamma}C_{j\delta}
                \bra{\alpha}H_{I}\ket{\beta}_{\text{AS}}
            \end{equation}
        Introducing the Lagrange multiplier and minimizing according to
        \Arf{sec:lagrange_multipliers} and using the orthogonality of the
        states we get the following constraint
            \begin{equation}
                \braket{p}{q} = \sum_{\alpha} C^{*}_{p\alpha}C_{p\alpha}
            \end{equation}
        giving the functional (Lagrangian)
            \begin{equation}
                \mathcal{L}\left[\psi^{\text{HF}}\right] = \EHF -
                \sum_{i\alpha} \varepsilon_iC^{*}_{p\alpha}C_{p\alpha}
            \end{equation}
        where $\varepsilon_i$ is the Lagrange multiplier which we give units
        energy for the dimensionality of the equations to match. The
        Euler-Lagrange equations is then obtained by minimization
            \begin{equation}
                \prd{C^{*}_{p\alpha}} \left[\EHF - \sum_{j\alpha}
                \varepsilon_jC^{*}_{p\alpha}C_{p\alpha}\right] =
                \prd{\varepsilon_p}[\mathcal{L}]
            \end{equation}
        Since the constraints are leveled functions, meaning they only scale
        the energy functional, we can effectively ignore the equation regarding
        $\varepsilon_p$\footnote{the analogy to this is basically that one can
        always choose the ground-point of a potential}.

\section{Quantum Monte Carlo\label{sec:QMC}}
    Quantum Monte Carlo, or QMC is a method for solving SchrÃ¶dinger's equation
    by a statistical approach using so-called \txtit{Markov Chain} simulations
    (also called random walk). The nature of the wave function at hand is
    fundamentally a statistical model defined on a large configuration space
    with small areas of densities. The Monte Carlo method is perfect for
    solving such a system because of the non-homogeneous distribution of
    calculation across the space. An standard approach with equal distribution
    of calculation would then yield a rather poor result with respect to
    computation cost.

    We will in this chapter address the Metropolis algorithm which is used to
    create a Markov chain and derive the equations used in the variational
    method.

    The chapter will use \txtit{Dirac Notation} \cite{GriffQuan} and all
    equations stated assume atomic units ($\hbar=m_e=e=4\pi\veps_0$) >> REF
    HERE ATOMIC UNITS <<.

    \subsection{The Variational Principle and Expectation Value of Energy}
        Given a Hamiltonian $\Ham$ and a trial wave function $\psiT$, the
        variational principle \cite{GriffQuan, NeOr} states that the
        expectation value of $\Ham$
            \begin{equation}
                E[\psi_T] = \ecp{\Ham} =
                \frac{\dinner{\psi_T}{\Ham}}{\pinner}
                \label{eq:ecpE}
            \end{equation}
        is an upper bound to the ground state energy
            \begin{equation}
                E_0 \leq \ecp{\Ham}
                \label{eq:ecpEBound}
            \end{equation}
        Now we can define our PDF as(see \Arf{susec:diffTHpdf} for a more
        detailed reasoning)
            \begin{equation}
                P(\mb{R}) \equiv \frac{\abs{\psi_T}^2}{\pinner}
                \label{eq:PDFdef}
            \end{equation}
        and with a new quantity
            \begin{equation}
                E_L(\mb{R};\mb{\alpha}) \equiv \frac{1}{\psiT}\Ham\psiT
                \label{eq:ELdef}
            \end{equation}
        the so-called local energy, we can rewrite \Arf{eq:ecpE} as
            \begin{equation}
                E[\psiT] = \ecp{E_L}
            \end{equation}
        The idea now is to find the lowest possible energy by varying a set of
        parameters $\mb{\alpha}$. The expectation value itself is found with
        the Metropolis algorithm, see \Arf{susec:MHAlg}.

    \subsection{The Trial Wave Function}
        The trial wave function is generally an arbitrary choice specific for
        the problem at hand, however it is in most cases favorable to expand
        the wave function in the eigenbasis (eigenstates) of the Hamiltonian
        since they forma complete set. This can be expressed as
            \begin{equation}
                \psiT = \sum_i C_i\psi_i(\mb{R};\mb{\alpha})
            \end{equation}
        with the $\psi_i$'s are the eigenstates of the Hamiltonian.

    \subsection{Use Diffusion Theory and the PDF\label{susec:diffTHpdf}}
        The statistics describing the expectation value states that any
        distribution may be applied in calculation, however if we take a close
        look at the local energy(\Arf{eq:ELdef}) we see that for all
        distributions the local energy is not defined at the zeros of $\psiT$.
        This means that an arbitrary PDF does not guarantee generation of
        points which makes $\psi_T=0$. This can be overcome by introducing the
        square of the wave function to be defined as the distribution function
        as given in \Arf{eq:PDFdef}.

        Because of the inherent statistical property of the wave function
        Quantum Mechanics can be modelled as a diffusion process, or more
        specifically, an \txtit{Isotropic Diffusion Process} which is
        essentially just a random walk model. Such a process is described by
        the Langevin equation with the corresponding Fokker-Planck equation
        describing the motion of the walkers(particles). See \cite{numstoch}
        for details.

    \subsection{Metropolis-Hastings Algorithm\label{susec:MHAlg}}
        The Metropolis algorithm bases itself on moves (also called
        transitions) as given in a Markov process. >> REF THIS HERE <<. This
        process is given by
            \begin{equation}
                w_i(t+\veps) = \sum_j\ufij{w}{i}{j}w_j(t)
            \end{equation}
        where $w(j\rarr i)$ is just a transition from state $j$ to state $i$.
        In order for the transition chain to reach a desired convergence while
        reversibility is kept, the well known condition for detailed balance
        must be fulfilled >> REF HERE DETAILED BALANCE <<. If detailed balance
        is true, then the following relations is true
            \begin{equation}
                w_i \ufij{T}{i}{j}\ufij{A}{i}{j} = w_j \ufij{T}{j}{i}\ufij{A}{j}{i}
                \Rarr \frac{w_i}{w_j} =
                \frac{\ufij{T}{j}{i}\ufij{A}{j}{i}}{\ufij{T}{i}{j}\ufij{A}{i}{j}}
                \label{eq:detailedBalance}
            \end{equation}
        We have here introduced two scenarios, the transition from
        configuration $i$ to configuration $j$ and the reverse process $j$ to
        $i$. Solving the acceptance $A$ for the two cases where the ratio in
        \ref{eq:detailedBalance} is either $1$(in which case the proposed state
        $j$ is accepted and transitions is made) and when the ratio is less
        then $1$. The Metropolis algorithm would in this case not automatically
        reject the latter case, but rather reject it with a proposed uniform
        probability. Introducing now a probability distribution function(PDF) $P$
        the acceptance $A$ can be expressed as
            \begin{equation}
                \ufij{A}{i}{j} =
                \text{min}\left(\frac{\ufij{P}{i}{j}}{\ufij{P}{j}{i}}
                \frac{\ufij{T}{i}{j}}{\ufij{T}{j}{i}} ,1\right)
                \label{eq:metropolisAcceptance}
            \end{equation}
        The so-called selection probability $T$ is defined specifically for
        each problem. For our case the PDF in question is the absolute square
        of the wave function and the selection $T$ is a Green's function
        derived in \Arf{susec:impSamp}.  The algorithm itself would then be
            \begin{enumerate}[label=(\roman*)]
                \item Pick initial state $i$ at random.
                \item Pick proposed state at random in accordance to
                    $\ufij{T}{j}{i}$.
                \item Accept state according to $\ufij{A}{j}{i}$.
                \item Jump to step (ii) until a specified number of states have
                    been generated.
                \item Save the state $i$ and jump to step (ii).
            \end{enumerate}

    \subsection{Importance Sampling\label{susec:impSamp}}
        Using the selection probability mentioned in \Arf{susec:MHAlg} in the
        Metropolis algorithm is called an \txtit{Importance sampling} because
        is essentially makes the sampling more concentrated around areas where
        the PDF has large values.

        In order to derive the form of this equation we use the statements
        presented in \Arf{susec:diffTHpdf}. With
            \begin{equation}
                \langevin
                \label{eq:langevin}
            \end{equation}
        the \txtit{Langevin equation} >>REF HERE LANGEVIN<< and apply Euler's
        method (Euler-Maryama >>REF<<) and obtain the new positions
            \begin{equation}
                \rnew = \rold + D\Fold\Delta t + \xi
                \label{eq:rnewdef}
            \end{equation}
        with the $r$'s being the new and old positions in the Markov chain
        respectively and $\Fold=F(\rold)$. The quantity $D$ is a diffusion
        therm equal to $1/2$ due to the kinetic energy(remind of natural units)
        and $\xi$ is a Gaussian distributed random number with $0$ mean and
        $\sqrt{\Delta t}$ variance.

        As mentioned a particle is described by the Fokker-Planck equation
            \begin{equation}
                \FokkerPlanck
                \label{eq:FokkerPlanckDef}
            \end{equation}
        With $P$ being the PDF(in current case the selection probability) and
        $F$ being the drift therm. In order to achieve convergence, that is a
        stationary probability density, we need the left hand side to be zero
        in \Arf{eq:FokkerPlanckDef} giving the following equation
            \begin{equation}
                \prd{x_i}[P][2] = P\prd{x_i}[\mb{F_i}] + \mb{F_i}\prd{x_i}[P]
            \end{equation}
        with the drift-therm being on the form $\mb{F}=g(x)\prtl P/\prtl x$ we
        finally have that
            \begin{equation}
                \mb{F} = \frac{2}{\psi_T}\nabla \psi_T
                \label{eq:qForceDef}
            \end{equation}
        This is the so-called \txtit{Quantum Force} which pushes the walkers
        towards regions where the wave function is large.
