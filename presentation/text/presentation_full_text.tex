\documentclass[10pt]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[style=numeric-comp, 
            backend=biber,
            url=false,
            doi=true,
            eprint=false]{biblatex}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{braket}
\usepackage{physics}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{Quantum Many-Body Simulations of Double Dot System}
% \date{\today}
\date{}
\author{Alocias Mariadason}
\institute{Institute of Physics}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction}

\begin{frame}[fragile]{Quantum-Dot}
    What is a quantum-dot?
        - small semiconductor nanostructure that confines motion of conduction.
        - Can be made with i.e electrostatic potentials.
        - $2$-$10$ nanometers in size
\end{frame}

\begin{frame}[fragile]{Quantum-Dot Model}
    Schrödinger equation $\mathcal{H}\ket{\psi} = E\ket{\psi}$
        - We model the quantum-dot as a many-body quantum system, meaning we solve the Schrödinger equation.
        - Let us express the hamiltonian
            \begin{itemize}
                \item $\mathcal{H} = - \frac{1}{2} \sum\limits_i \nabla^2_i +
                    \sum_{i<j} f\left(\blds{r}_j, \blds{r}_j\right) -
                    \frac{1}{2} \sum\limits_k \frac{\nabla^2_k}{M_k} + \sum\limits_{k<l}
                    g\left(\blds{R}_k,\blds{R}_l\right) +
                    V\left(\blds{R},\blds{r}\right)$
            \end{itemize}
            - This is the full system with a set of nuclei given here with an
            interaction potential $g$, kinetic for electrons, kinetic for
            nuclei an electron-electron interaction potential and lastly a
            confinement potential.
        - Born-Oppenheimer Approximation
            - Firstly we assume the nuclei to be far heavier than the electrons
            which means the system can be viewed as electrons moving in a
            vicinity around stationary nuclei.
            - The kinetic term then vanishes and the nuclei-interaction is a
            constant which can be ignored since adding a constant to the
            potential only alters the eigenvalues by a constant factor, the
            eigenfunctions remain unchanged.
            - We still need to model the electron-electron interaction and the
            confinement potential
\end{frame}

\begin{frame}[fragile]{Quantum-Dot Model}
    How do we model them?
        - Interaction: is the Coulomb repulsion
        - And for the confinement we studied two ways from >> ref <<. One is a simple parabolic-dot modelled by a harmonic oscillator.
        - Other is a double-dot, which is just a harmonic oscillator displaced in the $x$-direction.
        - Notice that for the confinement we have no terms including the nuclei. This is one of the characteristics of quantum-dot systems, we dont just set the nuclei interaction to constant, we also effectively ignore the entire thing (#nuclei have feeling)
\end{frame}

\section{Methods}
- In order to solve the many-body time-independent Schrödinger equation we need
some methods. We have used two method,
\begin{frame}[fragile]{Methods}
    Hartree-Fock(HF)
    Variational Monte-Carlo(VMC)
    - Before we explain the Hartree-Fock method it is worth-while to mention the Variational principle
\end{frame}

\begin{frame}[fragile]{Methods: Variational Principle}
    - It states this
    \begin{equation*}
        E_0 \leq \ecp{\mathcal{H}} =
        \frac{\Braket{\Psi|\mathcal{H}|\Psi}}{\Braket{\Psi|\Psi}}
        \label{eq:varPrinc}
    \end{equation*}
    - It basically says that given a system described by some Hamiltonian $\mathcal{H}$ the expectation value of the energy can NEVER be lower than the true ground-state energy of the system $E_0$. This might seem trivial, but it gives us the oppurtunity to define a trial wavefunction $\Psi$ for the the system and then minimize this. This simple idea is the main fundament on which both the Hartree-Fock method and the Variational Monte-Carlo method rests on.
\end{frame}

\begin{frame}[standout]{Methods: Slater Determinant And Energy Functional}
    Pauli Principle
        - The Pauli principle states that for systems of identical particles no same particle can occupy the same quantum state simultaneously.
    Slater Determinant
        - Following this result the many-body wavefunction for a fermionic
        system to be anti-symmetric and represented by a Slater determinant.
        - The P-operator indicates permuations over all possible $\psi_i$ and
        the latter case is the symmetric case present for bosons.
        - Taking the anti-summetric wavefunction as the ansatz for our
        trial-wavefunction we end up with the following energy-functional
        - The H-zero term is just the single-body Hamiltonian
        - This is the start of the Hartree-Fock method.
\end{frame}

\begin{frame}[standout]{Methods: Hartree-Fock}
    Assumptions
        - The first is already assumed and explained
        - We also restrict the system to not be relativistic. This is true for the low number of particles we work with.
        - The single-slater wavefunction assumption holds as long as we work
        with closed-shell systems. This means that the number of particles
        always fulfills the degeneracy levels, for the HO we have this figure.
        So we we can only run with $N=2,6,12,20$ and so on for $2D$ and
        $N=2,8,20$ for $3D$ for HO-system.
        - An for the MFA, this essentially means that the entire interaction
        potential can be described as a mean potential rather than specific
        individual point-potentials for each particle. This is fulfilled with
        the Coulomb interaction.
\end{frame}

\begin{frame}[standout]{Methods: Hartree-Fock}
    Contrained Minimization
        - As mentioned we need to minimize the energy. We notice one
        constraint, the spin orthogonality. We can minimize with the Lagrange
        multiplier method with the following Lagrangian. The delta-E part is
        the energy variation we seek the zero of.
        - We then define a Fock-operator giving the eigenvalue equation, and
        two operators defined from the energy-functional.
        - We then have $N+1$ equations to be solved
\end{frame}

\begin{frame}[fragile]{Methods: Hartree-Fock}
    Integrate out spin
        - The $\psi$'s have till now all been spin-orbitals. That is a
        wavefunction consisting of a spacial function multiplied by a spin
        function. It is convenient to integrate the spin dependency out of the
        eigenvalue equation. There exists two ways of achieving this.
    Pair spins as
        - where $\phi$ is the spacial part and $\alpha$ and $\beta$ are two
        spin configurations, up and down.
        - We restrict both the spin-configurations to have the same spacial
        function.
    Expand
        - and expand the spacial part in some known basis $\xi$ (i.e HO)
    Roothan-Hall
        - and end up with the Roothan-Hall equation with the elements of the
        Fock-matrix as this, the sum now only goes up-to $N/2$ because of the
        spin pairing
    Poople-Nesbet
        - If we dont restrict the spin-orbitals we end up with two Roothan-Hall
        equations and a Fock-Matrix with mixed elements.
    - The equations are a set of non-linear equations and have to be solvet
    iteratively until a convergence is reached.
\end{frame}

\begin{frame}[fragile]{Methods: Hartree-Fock}
    - The algorithm is to first pre-build the two-body, one-body and overlap
    matrices. The initial coefficients can be set to unit-matrix if nothing
    else is known. Set the Fock matrix either the restricted or the
    unrestricted form. Perform an optional mixing of $F$. Solve the generalized
    eigenvalue problem, calculate density matrix, mix this if desired, check
    for convergence. If reached calculate energy and end, if not save
    eigenvalues and density matrix (for mixing).
\end{frame}

\begin{frame}[fragile]{Methods: Variational Monte-Carlo}
    Variational Principle
        - Again we make use of the variational principle
        - However the variational method applies a statistical approach to the
        problem, we first rewrite the expectation value with a so-called local
        energy defined as this. The form of the integral is recognized as an
        integral over the probability distribution $\abs{\Psi}^2$.
   Metropolis-Hastings
        - We can solve this with the Metropolis algorithm in which we choose
        positions $\blds{R}$ randomly and accept accoring to this
        - The $T$-ratio is defined by Importance Sampling. We will skip the
        derivation, but the main idea is to use the fact that the quantum
        mechanical system can be modelled as an isotropic diffusion process,
        solve the Focker-Plank and Langevin equations and end up with a new
        proposal for the positions defined by a quantum force which pushes the
        particles towards regions where the wavefunction is large. $\xi$ is a
        normal distributed number with mean $0$ and unit variance
\end{frame}

\begin{frame}[fragile]{Methods: Variational Monte-Carlo}
    Algortihm (figure)
        - Initialize state, i.e set positions and calculate wavefunction,
        gradient, laplacian, local energy etc. Pick a random state accoring to
        the acceptance definition, either accept or reject. If we have enough
        samples, end, and if not keep accumulating.
\end{frame}

\section{Wavefunction}
    - We have now seen the Hartree-Fock and Variational Monte-Carlo methods,
    but one part of both has been ignored. Namely the choice of the
    wavefunction.

\begin{frame}[standout]{Methods: Wavefunction}
    - For the Hartree-Fock method we had the following
\end{frame}

\begin{frame}[standout]{Methods: Wavefunction}
    - We need to make a choice for this (focus red $\xi$)
\end{frame}

\begin{frame}[standout]{Wavefunction: Integral Elements}
    - The overlap, potential, kinetic and interaction integrals. We need to
    make a choice for the basisfunctions in order to find an analytic solution
    to these integrals.
\end{frame}

\begin{frame}[fragile]{Wavefunction: Single-Well}
    - One choice for the single-well case is the Hermite-functions which form
    an eigenbasis for the quantum harmonic oscillator system.
    - A solution in polar-coordinates already exists, however our goal is to
    actually use the Hartree-Fock basis in the variational method. The full
    wavefunction has to be evaluated in that case and the wavefunction in polar
    has a complex part which complicated the calculations.
    - In order to avoid that we went full Cartesian instead. Unfortunatly a
    solution did not exist, so we calculated it by using the fact that
    Hermite-Functions consist of Hermite-Gaussian constituents, with $C$ being the
    hermite coefficients.
    - The expressions to be solved are thus integrals over Hermite-Gaussian
    functions. A solution in $3D$ already exists for such integrals, and we
    used a similar approach and found the solution to be
\end{frame}

\begin{frame}[fragile]{Wavefunction: Single-Well Integral Elements}
    - We have a recursion relation for the coefficient $E$ and integrals $\xi$,
    $\zeta$ integrals have to be solved numerically and can be solved accuratly
    with Gaussian-Quadrature. In particular the $2D$ case follows the form of
    Gauss-Chebyshev quadrature. Again not to important to understand this
    fully, it is presented to show that solving the integral over
    Hermite-Functions is possible in Cartesian coordinates.
    - This is all that is needed for the Hartree-Fock method for the
    single-well case. For the double-well we still need to build a basis.
\end{frame}

\begin{frame}[fragile]{Wavefunction: Double-Well}
    - Notice that double-well is only a perturbation of single-well
    - Reasonable to use HO-functions as basis in an expansion
    - Projecting with the HO-bra we get an eigenvalue equation to be solved for
    the coefficients
    - And we have the following integral-elements. The eigenvalues DW are
    readily available from the solution to the eigenvalue equation and the
    two-body integral elements are also ready from the full solution of the
    harmonic oscillator system, meaning we only need to assemble the elements
    for Hartree-Fock.
\end{frame}

\begin{frame}[fragile]{Wavefunction: Slater Determinant}
    - For the variational method we have everything needed to actually build a
    trial-wavefunction.
    - The wavefunction is represented as a Slater determinant
    - We used three of them, first is the HO-basis with a variational parameter
    $\alpha$. This one was used as a benchmark to test the implementation.
    - The second is a Hartree-Fock basis.
    - and the third is a Hartree-Fock basis with a similar $\alpha$-parameter
    - For the correlation part a so-called Jastrow function is introduced. The
    ones we used are these
    - The Pad\'e-Jastrow with $\beta$ a variational parameters, NQS built from
    a restricted-boltzmann machine where $a$, $b$ and $w$ are variational
    parameters and a combination of both.
    - The reason for using the Pad\'e function is due to a cusp-condition. This
    essentially ensures that the wavefunction does not have a singularity when
    two particles are close, that is when $r_{ij}$ is zero, the NQS does not
    have this property as it does not depend on the inter-particle distance,
    however the functional form is more flexible. The hope then with the
    combination is that with the cusp-condition fulfilled by the
    Pad\'e-function we can model other correlations with the NQS.
    - The full wavefunction is then the Slater determinant multiplied with one
    of the correlation functions.
\end{frame}

\section{Implementation}

\begin{frame}[fragile]{Implementation}
    \CC
        - All calculations are implemented in C++ using Eigen, a linear algebra library
        - The reason for this is performance
        - Generalization with object-oriented programming, especially with the
        fact that the double-well reuses many of the elements from the
        single-well used as a benchmark. This gave us the oppurtunity to
        quickly implement the double-well system after the harmonic oscillator
        was implemented by just inheriting the harmonic oscillator class
\end{frame}

\begin{frame}[fragile]{Implementation: Cartesian}
    - The structure of the energy-levels of the Harmonic oscillator had to be
    respected, that. This structure here, as we have seen, had to be respected.
    The levels here are implemented in a class Cartesian and is used to
    pre-build a matrix containing these states.
\end{frame}

\begin{frame}[fragile]{Implementation: Hartree-Fock}
    - We have split the implementation in several parts. The main ones being
    HartreeFockSolver
        - Where the restricted Hartree-Fock method is implemented
    Integrals
        - GaussianIntegrals and DoubleWell
        - The Both of these inherit HartreeFockSolver, and DoubleWell inherits
        GaussianIntegrals additionally
    Hexpander: Recurrence relations 
        - The recurrence relations are contained within a class Hexpander, also
        inherited from in the integrals classes.
        - The class calculated and tabulates the coefficients and integral
        elements when initialized. The integral class does this with a function
        initializeParameters which is called once. The two-body elements are
        then built by fetching the elements from the Hexpander class.
    Parallelization
        - The two-body element is expensive to calculate, even with the
        pre-calculations and everything. In order to not waste to-much time, we
        parallelized the entire procedure. One problem however was that sending
        chunks of the two-body matrix in order to each process results in the
        latter processes calculating much more than the first few. We over-came
        this by weighting the chunck-sizes according to this
        - $S_i$ indicates the number of elements process $i$ calculates, the
        chunk size, and it is weighted by the sum product of the quantum
        numbers in its chunk. We first distribute the chunk sizes evenly,
        resulting in uneven computational strain, and then vary the sizes such
        that the mean of all of these $S_i$'s are as equal as possible.
        - Each process then calculates its own elements and sends it to root
        and then assembled into the full matrix, and the
        Hartree-Fock algorithm only run on one process
            - The Hartree-Fock algorithm is only run on root process
    Tabulation of Two-Body matrix
        - The two-body matrix is written to binary-file and read in for
        subsequent runs. This means we only need to run the heavy calculation
        once and just reuse this later.
\end{frame}

\begin{frame}[fragile]{Implementation: Variational Monte-Carlo}
    - We have the following wavefunction classes for modified harmonic
    oscillator, Hartree-Fock with Harmonic oscillator functions, the modified
    version of that and the two for double-well. All of those inherit from the
    same Cartesian class as before.
    - The Hermite class is generated with Python, that is we calculate the
    Hermite polynomials using the recurrence relation for Hermites symbolically
    with SymPy and write this to C++ template functions and write those to
    file.
    - DWC is same as before.
    - The implementation of the Jastrow class are given in these 4 classes, the
    main two classes which use the wavefunction classes are Slater and
    SlaterJastrow. Slater contains the Slater matrix along with inversers etc.
    and has functions for updating positions according to if the state was
    accepted or rejected.
    - On a technical note, Slater uses only one of the Wavefunction classes,
    this information is provided at compile time. The same goes for
    SlaterJastrow with the Jastrow classes. Additionally, Slater checks the
    wavefunction class in question for a set of functions at compile time
\end{frame}

\begin{frame}[fragile]{Implementation: Variational Monte-Carlo}
    Specifically, these functions. If they exist in the wavefunction class,
    they are compiled and called when the same function is called in Slater
    during the sampling. This gives the oppurtunity for specific factors or
    tabulations can be initialized and updated during the sampling giving room
    for optimizations. Of course, the updating and such has to be controlled by
    the user.
\end{frame}

\begin{frame}[fragile]{Implementation: Variational Monte-Carlo}
    The VMC class uses the Slater and SlaterJastrow classes for transition of
    states in the Metropolis sampling and depending on which type of sampling
    is used either BruteForce is initialized or ImportanceSampling is
    initialized. The only function that VMC uses is then the sampler function
    implemented in BruteForce and ImportanceSampling and smples accordingly.
    The Minimizer class can be used for the minimization, in that case it needs
    to be initialized with the specific Slater og SlaterJastrow class aswell.
    It then uses the VMC class for sampling. The derivations with respect to
    the variational parameters, which are needed in the minimization is also
    calcualted within VMC by using functions present in Slater, SlaterJastrow
    and the Jastrow and Wavefunction classes.
\end{frame}

\begin{frame}[fragile]{Implementation: Variational Monte-Carlo}
    A script which generated a template Wavefunction class is also present. It
    creates a header and source file with the neccesary function used by
    Slater. These are just the analytic expressions such as the gradient and
    laplacian of the wavefunction and also derivatives with respect to
    variational parameters. Currently one has to modify the generated C++ files
    manually, however the whole tweaking can be automated with for instance
    SymPy and PyBind.
\end{frame}

\section{Summary and Conclusion}

{\setbeamercolor{palette primary}{fg=black, bg=white}
\begin{frame}[standout]
  Questions?
\end{frame}
}

\appendix

\begin{frame}[fragile]{Questions}
\end{frame}

\begin{frame}[allowframebreaks]{References}

  \bibliography{demo}
  \bibliographystyle{abbrv}

\end{frame}

\end{document}
